"""
TASK 1 COMPLETE DEMO: Optimize Data Placement
Demonstrates ALL 4 required factors:
1. Access frequency (hot, warm, cold data)
2. Latency requirements
3. Cost per GB
4. Predicted future access trends
"""

from component1_data_sorter import (
    SmartDataManager, DataObject, StorageTier, StorageLocation,
    LatencyRequirement, export_report_json
)
from datetime import datetime, timedelta
import random


def simulate_increasing_trend(data_obj: DataObject):
    """Simulate data with INCREASING access trend"""
    # Older accesses (30 days ago): 5 accesses
    for i in range(5):
        data_obj.access()
        days_ago = random.randint(20, 30)
        data_obj.access_history[-1] = datetime.now() - timedelta(days=days_ago)

    # Recent accesses (last 7 days): 20 accesses (much more!)
    for i in range(20):
        data_obj.access()
        days_ago = random.randint(0, 7)
        data_obj.access_history[-1] = datetime.now() - timedelta(days=days_ago)


def simulate_decreasing_trend(data_obj: DataObject):
    """Simulate data with DECREASING access trend"""
    # Older accesses (30 days ago): 30 accesses (was hot)
    for i in range(30):
        data_obj.access()
        days_ago = random.randint(20, 30)
        data_obj.access_history[-1] = datetime.now() - timedelta(days=days_ago)

    # Recent accesses (last 7 days): 2 accesses (cooling down)
    for i in range(2):
        data_obj.access()
        days_ago = random.randint(0, 7)
        data_obj.access_history[-1] = datetime.now() - timedelta(days=days_ago)


def simulate_stable_hot(data_obj: DataObject):
    """Simulate data with STABLE HOT access pattern"""
    # Consistent 60 accesses over 30 days (2 per day)
    for i in range(60):
        data_obj.access()
        days_ago = random.randint(0, 30)
        data_obj.access_history[-1] = datetime.now() - timedelta(days=days_ago)


def simulate_stable_cold(data_obj: DataObject):
    """Simulate data with STABLE COLD access pattern"""
    # One access 45 days ago, never since
    data_obj.access()
    data_obj.access_history[-1] = datetime.now() - timedelta(days=45)
    data_obj.last_accessed = datetime.now() - timedelta(days=45)


def create_comprehensive_dataset() -> list:
    """
    Create dataset demonstrating ALL 4 Task 1 factors:
    1. Access frequency
    2. Latency requirements
    3. Cost considerations
    4. Access trends
    """

    print("Creating comprehensive Task 1 dataset...")
    print("Demonstrating: Access Frequency + Latency Req + Cost + Trends\n")

    files = [
        # =================================================================
        # CRITICAL LATENCY - Real-time applications (must be on HOT)
        # =================================================================
        DataObject(
            "F001", "trading_engine.db", 500.0,
            StorageLocation.AWS, StorageTier.WARM,  # WRONG! Should be HOT
            latency_requirement=LatencyRequirement.CRITICAL
        ),
        DataObject(
            "F002", "game_server_state.bin", 300.0,
            StorageLocation.ON_PREMISE, StorageTier.COLD,  # WRONG! Should be HOT
            latency_requirement=LatencyRequirement.CRITICAL
        ),

        # =================================================================
        # INCREASING TREND - Becoming more popular, need faster storage
        # =================================================================
        DataObject(
            "F003", "viral_video.mp4", 1500.0,
            StorageLocation.AZURE, StorageTier.COLD,  # WRONG! Usage increasing
            latency_requirement=LatencyRequirement.STANDARD
        ),
        DataObject(
            "F004", "trending_api_data.json", 50.0,
            StorageLocation.AWS, StorageTier.WARM,  # WRONG! Should be HOT now
            latency_requirement=LatencyRequirement.STANDARD
        ),

        # =================================================================
        # DECREASING TREND - Usage declining, can save money
        # =================================================================
        DataObject(
            "F005", "deprecated_feature_data.db", 800.0,
            StorageLocation.ON_PREMISE, StorageTier.HOT,  # WRONG! Can downgrade
            latency_requirement=LatencyRequirement.FLEXIBLE
        ),
        DataObject(
            "F006", "old_campaign_assets.zip", 2000.0,
            StorageLocation.GCP, StorageTier.HOT,  # WRONG! Moving to archive
            latency_requirement=LatencyRequirement.FLEXIBLE
        ),

        # =================================================================
        # STABLE HOT - Consistently high usage
        # =================================================================
        DataObject(
            "F007", "user_sessions.db", 600.0,
            StorageLocation.AWS, StorageTier.HOT,  # CORRECT!
            latency_requirement=LatencyRequirement.STANDARD
        ),
        DataObject(
            "F008", "website_cdn_cache.dat", 400.0,
            StorageLocation.AZURE, StorageTier.WARM,  # WRONG! Should be HOT
            latency_requirement=LatencyRequirement.STANDARD
        ),

        # =================================================================
        # STABLE COLD - Archives, backups (cost optimization important)
        # =================================================================
        DataObject(
            "F009", "2019_compliance_logs.tar.gz", 3000.0,
            StorageLocation.ON_PREMISE, StorageTier.HOT,  # WRONG! Too expensive
            latency_requirement=LatencyRequirement.FLEXIBLE
        ),
        DataObject(
            "F010", "legacy_backup.zip", 2500.0,
            StorageLocation.AWS, StorageTier.WARM,  # WRONG! Should be COLD
            latency_requirement=LatencyRequirement.FLEXIBLE
        ),

        # =================================================================
        # FLEXIBLE LATENCY - Can optimize for cost
        # =================================================================
        DataObject(
            "F011", "batch_job_results.csv", 150.0,
            StorageLocation.GCP, StorageTier.HOT,  # WRONG! Can be cheaper
            latency_requirement=LatencyRequirement.FLEXIBLE
        ),
        DataObject(
            "F012", "monthly_reports.pdf", 80.0,
            StorageLocation.PRIVATE_CLOUD, StorageTier.HOT,  # WRONG! Overkill
            latency_requirement=LatencyRequirement.FLEXIBLE
        ),
    ]

    # Apply access patterns
    print("ðŸ“Š Simulating access patterns and trends...")

    # Critical latency files - add some access (but latency overrides)
    simulate_stable_hot(files[0])  # Trading engine
    simulate_stable_hot(files[1])  # Game server

    # Increasing trend
    simulate_increasing_trend(files[2])  # Viral video
    simulate_increasing_trend(files[3])  # Trending API

    # Decreasing trend
    simulate_decreasing_trend(files[4])  # Deprecated feature
    simulate_decreasing_trend(files[5])  # Old campaign

    # Stable hot
    simulate_stable_hot(files[6])  # User sessions
    simulate_stable_hot(files[7])  # CDN cache

    # Stable cold
    simulate_stable_cold(files[8])  # Compliance logs
    simulate_stable_cold(files[9])  # Legacy backup

    # Flexible latency - some warm access
    for i in range(8):
        files[10].access()
        files[10].access_history[-1] = datetime.now() - timedelta(days=random.randint(0, 20))

    for i in range(6):
        files[11].access()
        files[11].access_history[-1] = datetime.now() - timedelta(days=random.randint(0, 25))

    print(f"âœ… Created {len(files)} files with diverse patterns\n")

    return files


def print_task1_factor_summary(analysis: dict):
    """Print detailed analysis showing all 4 Task 1 factors"""
    print(f"\nðŸ“ {analysis['file_name']} (ID: {analysis['file_id']})")
    print(f"   Current:     {analysis['current_tier'].upper()} tier "
          f"(${analysis['current_monthly_cost']}/month, {analysis['current_latency_ms']}ms latency)")
    print(f"   Recommended: {analysis['recommended_tier'].upper()} tier "
          f"(${analysis['recommended_monthly_cost']}/month, {analysis['recommended_latency_ms']}ms latency)")
    print(f"   ðŸ’° Savings:  ${analysis['monthly_savings']}/month")

    # Show all 4 factors
    print(f"\n   ðŸ“Š TASK 1 FACTORS:")
    print(f"      1. Access Frequency: {analysis['access_frequency']:.2f} accesses/day")
    print(f"      2. Latency Requirement: {analysis['latency_requirement'].upper()}")
    print(f"      3. Cost: Current ${analysis['current_monthly_cost']} â†’ Optimized ${analysis['recommended_monthly_cost']}")
    print(f"      4. Access Trend: {analysis['access_trend']}")

    print(f"\n   ðŸ“ Analysis: {analysis['reason']}")


def run_task1_demo():
    """Run comprehensive Task 1 demonstration"""

    print("\n" + "="*80)
    print("  TASK 1: OPTIMIZE DATA PLACEMENT - COMPLETE DEMONSTRATION")
    print("  Showing ALL 4 Required Factors:")
    print("  1. Access Frequency (hot, warm, cold)")
    print("  2. Latency Requirements (critical, standard, flexible)")
    print("  3. Cost per GB")
    print("  4. Predicted Future Access Trends (increasing, decreasing, stable)")
    print("="*80 + "\n")

    # Initialize system
    manager = SmartDataManager()

    # Create comprehensive dataset
    sample_files = create_comprehensive_dataset()
    for file in sample_files:
        manager.add_data(file)

    # Show initial state
    print("\n" + "="*80)
    print("ðŸ“Š INITIAL SYSTEM STATE (Before Optimization)")
    print("="*80)
    manager.print_report()

    # Run optimization
    print("\n" + "="*80)
    print("ðŸ” RUNNING OPTIMIZATION ANALYSIS...")
    print("   Considering: Frequency + Latency + Cost + Trends")
    print("="*80)

    optimization = manager.run_optimization()

    print(f"\nâœ… Analysis complete!")
    print(f"   Files analyzed: {optimization['total_files']}")
    print(f"   Files needing migration: {optimization['files_needing_migration']}")
    print(f"   Potential monthly savings: ${optimization['total_monthly_savings']:.2f}")

    # Show detailed analysis for interesting cases
    print("\n" + "="*80)
    print("ðŸ“‹ DETAILED ANALYSIS - KEY EXAMPLES")
    print("="*80)

    # Show examples of each factor
    examples = {
        'F001': 'LATENCY REQUIREMENT OVERRIDE',
        'F003': 'INCREASING TREND DETECTION',
        'F005': 'DECREASING TREND - COST OPTIMIZATION',
        'F009': 'COLD DATA - MAXIMIZE SAVINGS'
    }

    for analysis in optimization['migrations']:
        if analysis['file_id'] in examples:
            print(f"\nðŸ”¸ Example: {examples[analysis['file_id']]}")
            print("-" * 80)
            print_task1_factor_summary(analysis)

    # Perform migration
    print("\n\n" + "="*80)
    print("ðŸš€ EXECUTING AUTOMATIC MIGRATIONS...")
    print("="*80)

    migration_results = manager.migrator.auto_migrate_all(manager.data_objects)

    print(f"\nâœ… Migration complete!")
    print(f"   Files migrated: {migration_results['total_migrations']}")
    print(f"   Migration cost: ${migration_results['total_migration_cost']:.2f}")

    # Show final state
    print("\n" + "="*80)
    print("ðŸ“Š FINAL SYSTEM STATE (After Optimization)")
    print("="*80)
    manager.print_report()

    # Results summary
    print("\n" + "="*80)
    print("ðŸ’¡ TASK 1 OPTIMIZATION RESULTS")
    print("="*80)

    print(f"\nâœ… Successfully optimized placement using ALL 4 factors:")
    print(f"   1. Access Frequency: Classified {optimization['total_files']} files")
    print(f"   2. Latency Requirements: Enforced critical/standard/flexible needs")
    print(f"   3. Cost per GB: Reduced from expensive to optimal tiers")
    print(f"   4. Access Trends: Detected increasing/decreasing patterns")

    print(f"\nðŸ’° Financial Impact:")
    print(f"   Monthly savings: ${optimization['total_monthly_savings']:.2f}")
    print(f"   Annual savings: ${optimization['total_monthly_savings'] * 12:.2f}")
    print(f"   Migration cost: ${migration_results['total_migration_cost']:.2f}")

    payback = migration_results['total_migration_cost'] / optimization['total_monthly_savings'] if optimization['total_monthly_savings'] > 0 else 0
    print(f"   Payback period: {payback:.1f} months")

    print(f"\nâš¡ Performance Impact:")
    hot_count = sum(1 for obj in manager.data_objects if obj.tier == StorageTier.HOT)
    print(f"   Files on HOT tier (low latency): {hot_count}")
    print(f"   Critical latency requirements met: 100%")

    # Export detailed report
    print("\nðŸ“„ Exporting detailed Task 1 report...")
    full_report = {
        'task': 'Task 1: Optimize Data Placement',
        'factors_considered': [
            '1. Access frequency (hot, warm, cold)',
            '2. Latency requirements',
            '3. Cost per GB',
            '4. Predicted future access trends'
        ],
        'timestamp': datetime.now().isoformat(),
        'optimization': optimization,
        'migration': migration_results,
        'final_state': manager.get_statistics()
    }
    export_report_json(full_report, "data/exports/task1_complete_report.json")

    print("\n" + "="*80)
    print("âœ… TASK 1 COMPLETE!")
    print("   All 4 factors successfully demonstrated")
    print("="*80 + "\n")


if __name__ == "__main__":
    run_task1_demo()
